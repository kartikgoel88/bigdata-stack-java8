
services:
  base:
    build:
      context: .
      dockerfile: Dockerfile.base
      args:
        JAVA_VERSION: ${JAVA_VERSION}
    image: bigdata-stack-base:latest
    # This service is only used for building, not running

  runtime:
    build:
      context: .
      dockerfile: Dockerfile.hadoop-spark-base
      args:
        BASE_IMAGE: bigdata-stack-base:latest
        HADOOP_HOME: ${HADOOP_HOME}
        HIVE_HOME: ${HIVE_HOME}
        SPARK_HOME: ${SPARK_HOME}
        HADOOP_VERSION: ${HADOOP_VERSION}
        HIVE_VERSION: ${HIVE_VERSION}
        SPARK_VERSION: ${SPARK_VERSION}
    image: bigdata-runtime:hadoop
    # Note: Base image must be built first before building runtime
    # Build order: docker-compose build base && docker-compose build runtime

  namenode:
    image: bigdata-runtime:hadoop
    container_name: hadoop-namenode
    hostname: ${NAMENODE_HOST}
    command: namenode
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      HIVE_USER: ${HIVE_USER}
      NAMENODE_RPC_PORT_CONTAINER: ${NAMENODE_RPC_PORT_CONTAINER}
      WAIT_RETRIES: ${WAIT_RETRIES}
    ports:
      - "${NAMENODE_WEB_PORT}:${NAMENODE_WEB_PORT_CONTAINER}"
      - "${NAMENODE_RPC_PORT}:${NAMENODE_RPC_PORT_CONTAINER}"
    volumes:
      - namenode-data:${HADOOP_HOME}/data/namenode
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - hadoop-logs:${HADOOP_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "hdfs dfsadmin -report >/dev/null 2>&1 || nc -z localhost ${NAMENODE_RPC_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  datanode:
    image: bigdata-runtime:hadoop
    hostname: datanode
    command: datanode
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "${DATANODE_PORT}:${DATANODE_PORT_CONTAINER}"
    volumes:
      - datanode-data:${HADOOP_HOME}/data/datanode
      - hadoop-tmp:${HADOOP_HOME}/data/tmp
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - hadoop-logs:${HADOOP_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${DATANODE_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  resourcemanager:
    image: bigdata-runtime:hadoop
    hostname: resourcemanager
    command: resourcemanager
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      - namenode
    ports:
      - "${RESOURCEMANAGER_PORT}:${RESOURCEMANAGER_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - hadoop-logs:${HADOOP_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${RESOURCEMANAGER_PORT_CONTAINER}/ws/v1/cluster/info >/dev/null 2>&1 || nc -z localhost ${RESOURCEMANAGER_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  nodemanager:
    image: bigdata-runtime:hadoop
    hostname: nodemanager
    command: nodemanager
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      resourcemanager:
        condition: service_healthy
      namenode:
        condition: service_healthy
    ports:
      - "${NODEMANAGER_PORT}:${NODEMANAGER_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - hadoop-logs:${HADOOP_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8042/node >/dev/null 2>&1 || jps | grep -i NodeManager || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  postgres:
    image: postgres:${POSTGRES_VERSION}
    hostname: ${POSTGRES_HOST}
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT_CONTAINER}"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/postgres/init-postgres.sh:/docker-entrypoint-initdb.d/01-init-databases.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks: [hadoop-network]
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  hive-metastore:
    image: bigdata-runtime:hadoop
    hostname: ${HIVE_METASTORE_HOST}
    container_name: hive-metastore
    command: metastore
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      HIVE_HOME: ${HIVE_HOME}
      HIVE_VERSION: ${HIVE_VERSION}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_PORT_CONTAINER: ${POSTGRES_PORT_CONTAINER}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      postgres:
        condition: service_healthy
      namenode:
        condition: service_healthy
    ports:
      - "${HIVE_METASTORE_PORT}:${HIVE_METASTORE_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - ./config/hive:${HIVE_CONF_DIR}
      - hive-logs:${HIVE_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${HIVE_METASTORE_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  hive-server:
    image: bigdata-runtime:hadoop
    hostname: hive-server
    container_name: hive-server
    command: hiveserver2
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      HIVE_HOME: ${HIVE_HOME}
      HIVE_VERSION: ${HIVE_VERSION}
      HIVE_USER: ${HIVE_USER}
      HIVE_METASTORE_HOST: ${HIVE_METASTORE_HOST}
      HIVE_METASTORE_PORT: ${HIVE_METASTORE_PORT}
      HIVE_METASTORE_PORT_CONTAINER: ${HIVE_METASTORE_PORT_CONTAINER}
      HIVESERVER2_PORT_CONTAINER: ${HIVESERVER2_PORT_CONTAINER}
      NAMENODE_HOST: ${NAMENODE_HOST}
      NAMENODE_RPC_PORT_CONTAINER: ${NAMENODE_RPC_PORT_CONTAINER}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      hive-metastore:
        condition: service_healthy
      namenode:
        condition: service_healthy
    ports:
      - "${HIVESERVER2_PORT}:${HIVESERVER2_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - ./config/hive:${HIVE_CONF_DIR}
      - hive-logs:${HIVE_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${HIVESERVER2_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  spark-master:
    image: bigdata-runtime:hadoop
    hostname: ${SPARK_MASTER_HOST}
    command: spark-master
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      SPARK_HOME: ${SPARK_HOME}
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "${SPARK_MASTER_PORT}:${SPARK_MASTER_PORT_CONTAINER}"
      - "${SPARK_MASTER_WEB_PORT}:${SPARK_MASTER_WEB_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - ./config/spark:${SPARK_CONF_DIR}
      - spark-logs:${SPARK_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${SPARK_MASTER_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  spark-worker:
    image: bigdata-runtime:hadoop
    hostname: spark-worker
    command: spark-worker
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      SPARK_HOME: ${SPARK_HOME}
      SPARK_VERSION: ${SPARK_VERSION}
      SPARK_MASTER: ${SPARK_MASTER}
      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
      SPARK_MASTER_PORT_CONTAINER: ${SPARK_MASTER_PORT_CONTAINER}
      NAMENODE_HOST: ${NAMENODE_HOST}
      NAMENODE_RPC_PORT_CONTAINER: ${NAMENODE_RPC_PORT_CONTAINER}
      WAIT_RETRIES: ${WAIT_RETRIES}
    depends_on:
      spark-master:
        condition: service_healthy
      namenode:
        condition: service_healthy
    ports:
      - "${SPARK_WORKER_PORT}:${SPARK_WORKER_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - ./config/spark:${SPARK_CONF_DIR}
      - spark-logs:${SPARK_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${SPARK_WORKER_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  spark-history-server:
    image: bigdata-runtime:hadoop
    hostname: spark-history-server
    command: spark-history
    restart: unless-stopped
    environment:
      HADOOP_HOME: ${HADOOP_HOME}
      SPARK_HOME: ${SPARK_HOME}
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "${SPARK_HISTORY_PORT}:${SPARK_HISTORY_PORT_CONTAINER}"
    volumes:
      - ./config/hadoop:${HADOOP_CONF_DIR}
      - ./config/spark:${SPARK_CONF_DIR}
      - spark-logs:${SPARK_HOME}/logs
    networks: [hadoop-network]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost ${SPARK_HISTORY_PORT_CONTAINER} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

volumes:
  namenode-data:
  datanode-data:
  hadoop-tmp:
  postgres-data:
  hadoop-logs:
  hive-logs:
  spark-logs:

networks:
  hadoop-network:
    driver: bridge
    name: bigdata-hadoop-network
