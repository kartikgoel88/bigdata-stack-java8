# Use the base image
ARG BASE_IMAGE=bigdata-stack-base:latest
FROM ${BASE_IMAGE}

# Set environment variables
ENV SPARK_VERSION=3.5.8
ENV HADOOP_VERSION=3.3.6
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/hadoop
ENV SPARK_CONF_DIR=$SPARK_HOME/conf
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

WORKDIR /opt

# Copy downloads directory into image (for local files during build)
COPY downloads /downloads
RUN mkdir -p /downloads || true

# Install Hadoop from local downloads folder (needed for HDFS access)
RUN set -e && \
    HADOOP_FILE_BASE=hadoop-${HADOOP_VERSION} && \
    HADOOP_FILE_TAR=${HADOOP_FILE_BASE}.tar && \
    LOCAL_DOWNLOADS=/downloads && \
    if [ -f "${LOCAL_DOWNLOADS}/${HADOOP_FILE_TAR}" ]; then \
        echo "Using local file: ${LOCAL_DOWNLOADS}/${HADOOP_FILE_TAR}"; \
        cp ${LOCAL_DOWNLOADS}/${HADOOP_FILE_TAR} ./; \
        tar -xf ${HADOOP_FILE_TAR} -C /opt && \
        mv /opt/${HADOOP_FILE_BASE} $HADOOP_HOME && \
        rm ${HADOOP_FILE_TAR}; \
    else \
        echo "ERROR: Hadoop file not found in downloads folder: ${LOCAL_DOWNLOADS}/${HADOOP_FILE_TAR}"; \
        echo "Please download hadoop-${HADOOP_VERSION}.tar to the downloads/ folder"; \
        exit 1; \
    fi

# Copy Hadoop configuration files
COPY config/hadoop/* $HADOOP_CONF_DIR/

# Install Python 3 and pip (required for PySpark)
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install Spark from local downloads folder (.tar extension)
RUN set -e && \
    SPARK_FILE_BASE=spark-${SPARK_VERSION}-bin-hadoop3 && \
    SPARK_FILE_TAR=${SPARK_FILE_BASE}.tar && \
    LOCAL_DOWNLOADS=/downloads && \
    if [ -f "${LOCAL_DOWNLOADS}/${SPARK_FILE_TAR}" ]; then \
        echo "Using local file: ${LOCAL_DOWNLOADS}/${SPARK_FILE_TAR}"; \
        cp ${LOCAL_DOWNLOADS}/${SPARK_FILE_TAR} ./; \
        tar -xf ${SPARK_FILE_TAR} -C /opt && \
        mv /opt/${SPARK_FILE_BASE} $SPARK_HOME && \
        rm ${SPARK_FILE_TAR}; \
    else \
        echo "ERROR: Spark file not found in downloads folder: ${LOCAL_DOWNLOADS}/${SPARK_FILE_TAR}"; \
        echo "Please download spark-${SPARK_VERSION}-bin-hadoop3.tar to the downloads/ folder"; \
        exit 1; \
    fi

# Commented out wget - using local downloads instead
# RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz && \
#     tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt && \
#     mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 $SPARK_HOME && \
#     rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

# Install PySpark via pip (optional - Spark already includes PySpark in $SPARK_HOME/python/pyspark)
# This allows using PySpark from standalone Python scripts without setting PYTHONPATH
# If pip install fails, Spark's included PySpark will still work
RUN pip3 install --no-cache-dir pyspark==${SPARK_VERSION} || \
    (echo "INFO: PySpark ${SPARK_VERSION} not available via pip, trying latest version..." && \
     pip3 install --no-cache-dir pyspark || \
     echo "INFO: PySpark pip install skipped - Spark's included PySpark is available at \$SPARK_HOME/python/pyspark") || true

# Create necessary directories
RUN mkdir -p /opt/spark/work && \
    mkdir -p /opt/spark/logs && \
    chmod -R 755 /opt/spark

# Copy configuration files
COPY config/spark/* $SPARK_CONF_DIR/

# Copy Hive configuration for Spark to access Hive metastore
COPY config/hive/hive-site.xml $SPARK_CONF_DIR/

# Copy PostgreSQL JDBC driver to Spark jars (needed for Hive metastore connection)
RUN mkdir -p $SPARK_HOME/jars && \
    set -e && \
    JDBC_FILE=postgresql-42.5.6.jar && \
    LOCAL_DOWNLOADS=/downloads && \
    if [ -f "${LOCAL_DOWNLOADS}/${JDBC_FILE}" ]; then \
        echo "Using local JDBC driver: ${LOCAL_DOWNLOADS}/${JDBC_FILE}"; \
        cp ${LOCAL_DOWNLOADS}/${JDBC_FILE} $SPARK_HOME/jars/; \
    else \
        echo "JDBC driver not found in downloads, downloading..."; \
        wget https://jdbc.postgresql.org/download/${JDBC_FILE} -P $SPARK_HOME/jars/; \
    fi

# Create log4j.properties if not provided (fallback)
RUN if [ ! -f "$SPARK_CONF_DIR/log4j.properties" ]; then \
        cp $SPARK_HOME/conf/log4j.properties.template $SPARK_CONF_DIR/log4j.properties 2>/dev/null || \
        (echo "log4j.rootCategory=WARN, console" > $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.appender.console=org.apache.log4j.ConsoleAppender" >> $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.appender.console.target=System.err" >> $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.appender.console.layout=org.apache.log4j.PatternLayout" >> $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n" >> $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.logger.org.apache.spark.repl.Main=WARN" >> $SPARK_CONF_DIR/log4j.properties && \
        echo "log4j.logger.org.apache.spark.SparkContext=WARN" >> $SPARK_CONF_DIR/log4j.properties); \
    fi

WORKDIR /opt

EXPOSE 7077 8080 8081 4040

CMD ["/bin/bash"]

