# Spark Configuration
# Default master - can be overridden with --master flag
# Options: local[*], yarn, spark://localhost:7077
# Using local[*] as default for faster interactive PySpark sessions
spark.master                     local[*]
spark.submit.deployMode          client
spark.driver.memory              2g
spark.executor.memory            2g
# Executor settings only apply to cluster modes (YARN/standalone)
# spark.executor.cores             2
# spark.executor.instances         2

# Hive Integration - enables Spark to read from Hive tables
spark.sql.catalogImplementation  hive
spark.sql.warehouse.dir          hdfs://namenode:9000/user/hive/warehouse
spark.hadoop.hive.metastore.uris  thrift://hive-metastore:9083

# YARN Configuration (only needed when using YARN mode)
# spark.yarn.archive               hdfs://namenode:9000/spark-libs/spark-libs.zip
# spark.yarn.jars                  hdfs://namenode:9000/spark-libs/*.jar

# HDFS Configuration
spark.hadoop.fs.defaultFS        hdfs://namenode:9000

# Event Log (enabled for cluster mode - disable for local mode if needed)
# Enable only when using YARN or standalone cluster
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://namenode:9000/spark-logs
spark.history.fs.logDirectory    hdfs://namenode:9000/spark-logs

# History Server UI Configuration
# Bind to 0.0.0.0 to allow access from outside container
spark.history.ui.port            18080
spark.history.ui.bindAddress     0.0.0.0

# PySpark Configuration
spark.pyspark.python             python3
spark.pyspark.driver.python      python3

# Serialization
spark.serializer                 org.apache.spark.serializer.KryoSerializer

# Driver bind address (for client mode applications)
# Use 0.0.0.0 to bind to all interfaces, allowing connections from other containers
spark.driver.bindAddress         0.0.0.0

# Dynamic Allocation (only for YARN mode)
# spark.dynamicAllocation.enabled  true
# spark.dynamicAllocation.minExecutors  1
# spark.dynamicAllocation.maxExecutors  4



