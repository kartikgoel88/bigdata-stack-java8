# Spark Configuration
# Default master - can be overridden with --master flag
# Options: local[*], yarn, spark://localhost:7077
# Using local[*] as default for faster interactive PySpark sessions
spark.master                     local[*]
spark.submit.deployMode          client
spark.driver.memory              2g
spark.executor.memory            2g
# Executor settings only apply to cluster modes (YARN/standalone)
# spark.executor.cores             2
# spark.executor.instances         2

# Hive Integration (optional, comment out if not using Hive)
# spark.sql.catalogImplementation  hive
# spark.sql.warehouse.dir          hdfs://namenode:9000/user/hive/warehouse

# YARN Configuration (only needed when using YARN mode)
# spark.yarn.archive               hdfs://namenode:9000/spark-libs/spark-libs.zip
# spark.yarn.jars                  hdfs://namenode:9000/spark-libs/*.jar

# HDFS Configuration
spark.hadoop.fs.defaultFS        hdfs://namenode:9000

# Event Log (disabled for local mode to avoid HDFS dependency)
# Enable only when using YARN or standalone cluster
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:9000/spark-logs
# spark.history.fs.logDirectory    hdfs://namenode:9000/spark-logs
spark.eventLog.enabled           false

# PySpark Configuration
spark.pyspark.python             python3
spark.pyspark.driver.python      python3

# Serialization
spark.serializer                 org.apache.spark.serializer.KryoSerializer

# Dynamic Allocation (only for YARN mode)
# spark.dynamicAllocation.enabled  true
# spark.dynamicAllocation.minExecutors  1
# spark.dynamicAllocation.maxExecutors  4

